# Research-Grade Federated HuBERT Distillation Configuration
# Methodology perfectly aligned with pretraining implementation
distillation:
  # Student model architecture
  student_hidden_size: 256  # Reduced from 384
  student_num_layers: 4     # Reduced from 6
  vocab_size: 504
  frame_stride: 320
  
  # Training parameters
  batch_size: 8             # Reduced from 16
  local_epochs: 10
  learning_rate: 5e-4
  weight_decay: 0.01
  
  # Distillation parameters
  temperature: 4.0
  alpha: 0.7
  beta: 0.3
  
  # Memory optimization
  gradient_accumulation_steps: 4  # Increased from 2
  mixed_precision: true
  pin_memory: true
  num_workers: 8            # Reduced from 16
  gradient_checkpointing: true
  use_cache: false
  
  # Federated learning
  num_rounds: 10
  min_fit_clients: 2
  min_evaluate_clients: 2
  min_available_clients: 2

data:
  dataset_name: 'librispeech'
  subset: ['train-clean-100', 'train-clean-360', 'train-other-500']
  partition_method: 'speaker_based'
  iid_distribution: false
  test_split: 0.1
  validation_split: 0.1
  partitioned_data_root: '/home/saadan/scratch/federated_librispeech/src/federated_librispeech/data'  # Updated to match pretraining
  
  # Data loading optimization - Resource-focused (matching pretraining)
  dataloader:
    num_workers: 8           # Reduced from 16
    pin_memory: true
    prefetch_factor: 4       # Reduced from 8
    drop_last: true
    shuffle: true

strategy:
  fraction_fit: 1.0
  fraction_evaluate: 1.0
  min_fit_clients: 2
  min_evaluate_clients: 2
  min_available_clients: 2

server:
  aggregation:
    weighted_averaging: true
    clip_norm: 1.0
  
  early_stopping:
    enabled: true
    monitor: 'eval_distillation_loss'
    patience: 2  # Reduced to match pretraining
    min_delta: 0.001

client:
  enable_gpu: true
  max_memory_usage: '8GB'   # Reduced from 16GB
  mixed_precision: true
  local_config:
    batch_size: 8            # Reduced from 16
    epochs: 10

checkpointing:
  # Checkpoint management strategy (matching pretraining exactly):
  # - Always saves latest model (latest_state.pt)
  # - Saves best model based on validation loss (best_state.pt)
  # - Saves round-specific checkpoints (round_XXX_state.pt)
  # - Keeps exactly 3 checkpoints: best, current round, and previous round
  save_dir: '/home/saadan/scratch/federated_librispeech/src/checkpoints/distillation'
  save_latest: true          # Always save latest model
  save_best: true            # Save best model based on validation loss
  save_best_round: true      # Save checkpoint for each round
  cleanup_old: true          # Remove old round checkpoints
  max_checkpoints: 3         # Keep exactly 3 checkpoints: best, current, previous

logging:
  log_dir: 'logs/distillation'
  client_log_dir: 'logs/distillation/clients'
  server_log_dir: 'logs/distillation/server'
  log_level: 'WARNING'  # Reduced from INFO to WARNING for performance (matching pretraining)
  client_log_level: 'WARNING'  # Reduced from INFO to WARNING for performance (matching pretraining)
  server_log_level: 'WARNING'  # Reduced from INFO to WARNING for performance (matching pretraining)
  save_metrics_to_file: true
  metrics_file: 'logs/distillation/federated_distillation_metrics.json'
  log_metrics_every_n_steps: 20  # Increased from 2 to 20 for performance (matching pretraining)

simulation:
  num_supernodes: 2
