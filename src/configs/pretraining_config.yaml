# Research-Standard Federated HuBERT Pretraining Configuration
# Following original HuBERT paper architecture and hyperparameters
# Optimized for Compute Canada environment (32GB RAM, 8 CPUs, 1 GPU)

pretraining:
  # Model configuration - HuBERT Base (research standard)
  hidden_size: 768                    # Standard HuBERT base size
  intermediate_size: 3072             # 4x hidden_size
  num_attention_heads: 12             # Standard for HuBERT base
  num_hidden_layers: 12               # Standard for HuBERT base
  vocab_size: 504                     # K-means vocabulary size from HuBERT paper
  
  # HuBERT-specific configurations (added for s3prl compatibility)
  extractor_mode: "default"
  activation_fn: "gelu"               # Changed from 'activation' to 'activation_fn' for s3prl
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.0
  final_dim: 256                      # HuBERT final projection dimension
  layer_norm_first: true              # HuBERT uses pre-norm
  conv_feature_layers: "[(512,10,5)] + [(512,3,2)]*4 + [(512,2,2)]*2"  # CNN feature extractor
  conv_bias: false
  encoder_layerdrop: 0.0
  dropout_input: 0.0
  dropout_features: 0.0
  feature_grad_mult: 0.1
  untie_final_proj: true
  
  # Masking configuration (critical for HuBERT pretraining)
  logit_temp: 0.1
  mask_prob: 0.08                     # Reduced from 0.65 to match HuBERT paper
  mask_selection: "static"
  mask_other: 0
  mask_length: 10
  no_mask_overlap: false
  mask_min_space: 1
  
  # Training configuration - Optimized for Compute Canada (32GB RAM, 1 GPU)
  batch_size: 8                       # Reduced from 16 for memory constraints
  local_epochs: 1                     # Changed from 10 to 1 for federated rounds
  learning_rate: 5e-4                 # Keep standard HuBERT learning rate
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Audio processing configuration
  max_audio_length: 250000           # Standard s3prl length (~15.6s at 16kHz)
  sample_rate: 16000
  label_rate: 50                     # HuBERT label rate (50Hz)
  
  # Task-specific configurations (for HubertPretrainingConfig)
  normalize: false
  enable_padding: false
  max_keep_size: null
  min_sample_size: null
  random_crop: true
  pad_audio: false
  
  # Performance optimization - Compute Canada optimized
  gradient_accumulation_steps: 4      # Increased to compensate for smaller batch_size
  mixed_precision: true
  pin_memory: true
  num_workers: 4                      # Reduced from 16 to 4 for 8-CPU allocation
  
  # Memory optimization
  gradient_checkpointing: true
  use_cache: false

# Simulation configuration (Flower federated learning)
simulation:
  num_supernodes: 10                  # Number of federated clients
  fraction_fit: 1.0                   # Fraction of clients to use for training
  fraction_evaluate: 1.0              # Fraction of clients to use for evaluation
  min_fit_clients: 2                  # Minimum clients for training (reduced for testing)
  min_evaluate_clients: 2             # Minimum clients for evaluation
  min_available_clients: 2            # Minimum available clients
  
  # Resource allocation (Compute Canada optimized)
  num_cpus: 4                         # Conservative CPU allocation
  num_gpus: 0.5                       # Share GPU between 2 clients max
  round_timeout: 1800                 # 30 minutes per round
  
  # FedAdam specific parameters
  eta: 0.001                          # Server learning rate
  eta_l: 0.001                        # Client learning rate (same as server)
  beta_1: 0.9                         # First moment decay
  beta_2: 0.99                        # Second moment decay
  tau: 1e-9                           # Adaptability control

# Federated learning configuration
federated:
  num_rounds: 10                      # Total federated training rounds
  strategy: "fedadam"                 # Federated optimization strategy
  
  # Client configuration
  client:
    local_epochs: 1                   # Epochs per client per round
    evaluate_locally: true
    evaluation_fraction: 0.1
    
  # Server configuration
  server:
    aggregation:
      weighted_averaging: true
      clip_norm: 1.0
    
    early_stopping:
      enabled: true
      monitor: 'train_loss'           # Monitor training loss
      patience: 3                     # Increased patience for federated setting
      min_delta: 0.001

# Data configuration
data:
  dataset_name: 'librispeech'
  subset: ['train-clean-100', 'train-clean-360', 'train-other-500']
  partition_method: 'speaker_based'
  iid_distribution: false
  test_split: 0.1
  validation_split: 0.1
  partitioned_data_root: '/home/saadan/scratch/federated_librispeech/src/federated_librispeech/data'
  
  # S3PRL-compatible LibriSpeech source paths
  librispeech_source: '/home/saadan/scratch/federated_librispeech/LibriSpeechTars/LibriSpeech'
  global_validation_path: 'dev-clean'
  global_test_path: 'test-clean'
  
  # Data loading optimization - Compute Canada optimized
  dataloader:
    num_workers: 4                    # Reduced for 8-CPU allocation
    pin_memory: true
    prefetch_factor: 4                # Reduced for memory efficiency
    drop_last: true
    shuffle: true

# Logging configuration
logging:
  log_dir: 'logs/pretraining'
  client_log_dir: 'logs/pretraining/clients'
  server_log_dir: 'logs/pretraining/server'
  log_level: 'INFO'                   # Changed back to INFO for debugging
  client_log_level: 'INFO'
  server_log_level: 'INFO'
  save_metrics_to_file: true
  metrics_file: 'logs/pretraining/federated_pretraining_metrics.json'
  log_metrics_every_n_steps: 10      # Reduced for better monitoring

# Checkpointing configuration
checkpointing:
  save_dir: '/home/saadan/scratch/federated_librispeech/src/checkpoints/pretraining'
  save_latest: false
  save_best: true
  save_best_round: true
  cleanup_old: true
  max_checkpoints: 3

# Environment-specific configuration for Compute Canada
environment:
  slurm_account: "def-aravila"
  allocated_time: "6:00:00"
  allocated_memory: "32G"
  allocated_cpus: 8
  allocated_gpus: 1
  
  # Virtual environment and modules
  venv_path: "flvenv/bin/activate"
  required_modules:
    - "StdEnv/2023"
    - "scipy-stack/2025a"
  
  # TMUX configuration for long-running jobs
  use_tmux: true
  tmux_session_name: "federated_hubert"

# Research Standards Compliance
# This configuration follows the original HuBERT paper and s3prl implementation:
# - All HuBERT-specific parameters match s3prl defaults
# - Masking parameters follow HuBERT paper (mask_prob: 0.08)
# - Audio processing matches s3prl standards (250000 samples = ~15.6s)
# - Model architecture is HuBERT Base standard
# - Federated learning parameters are optimized for research comparison
