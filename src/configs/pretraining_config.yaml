# Research-Standard Federated HuBERT Pretraining Configuration
# Following original HuBERT paper architecture and hyperparameters
pretraining:
  # Model configuration - HuBERT Base (research standard)
  hidden_size: 768                    # Standard HuBERT base size (768/12=64 attention heads)
  intermediate_size: 3072             # 4x hidden_size (768*4=3072)
  num_attention_heads: 12            # Standard for HuBERT base
  num_hidden_layers: 12              # Standard for HuBERT base
  vocab_size: 504                    # K-means vocabulary size from HuBERT paper
  frame_stride: 320                  # Frame stride for audio processing
  dropout: 0.1                       # Standard dropout rate
  activation: "gelu"                 # HuBERT uses GELU activation
  layer_norm_eps: 1e-12             # Standard layer norm epsilon
  initializer_range: 0.02            # Standard weight initialization
  
  # Training configuration - Optimized for resource utilization
  batch_size: 16  # Increased for better GPU utilization and training efficiency
  local_epochs: 10
  learning_rate: 5e-4
  weight_decay: 0.01
  max_grad_norm: 1.0
  
  # Data configuration
  max_audio_length: 40000
  sample_rate: 16000
  mask_length: 10
  mask_prob: 0.08
  
  # Performance optimization - Resource-focused
  gradient_accumulation_steps: 2  # Reduced since we increased batch_size
  mixed_precision: true
  pin_memory: true  # Enabled for better GPU performance
  num_workers: 16  # Increased for better CPU utilization (16 cores available)
  
  # Memory optimization
  gradient_checkpointing: true  # Enable to trade compute for memory
  use_cache: false  # Disable KV cache to save memory
  
  # Federated learning
  num_rounds: 10
  min_fit_clients: 10
  min_evaluate_clients: 10
  min_available_clients: 10
  fraction_fit: 1.0
  fraction_evaluate: 1.0

data:
  dataset_name: 'librispeech'
  subset: ['train-clean-100', 'train-clean-360', 'train-other-500']
  partition_method: 'speaker_based'
  iid_distribution: false
  test_split: 0.1
  validation_split: 0.1
  partitioned_data_root: '/home/saadan/scratch/federated_librispeech/src/federated_librispeech/data'
  
  # Data loading optimization - Resource-focused
  dataloader:
    num_workers: 16  # Increased for better CPU utilization (16 cores available)
    pin_memory: true  # Enabled for better GPU performance
    prefetch_factor: 8  # Increased for better data loading
    drop_last: true
    shuffle: true

server:
  aggregation:
    weighted_averaging: true
    clip_norm: 1.0
  
  early_stopping:
    enabled: true
    monitor: 'eval_pretrain_loss'
    patience: 2
    min_delta: 0.001

client:
  enable_gpu: true
  max_memory_usage: '16GB'  # Increased for better performance
  mixed_precision: true  # Enable for memory efficiency
  evaluate_locally: true
  evaluation_fraction: 0.1
  
  local_config:
    batch_size: 16  # Increased to match pretraining.batch_size
    epochs: 1
    shuffle: true
    drop_last: true

logging:
  log_dir: 'logs/pretraining'
  client_log_dir: 'logs/pretraining/clients'
  server_log_dir: 'logs/pretraining/server'
  log_level: 'WARNING'  # Reduced from INFO to WARNING for performance
  client_log_level: 'WARNING'  # Reduced from INFO to WARNING for performance
  server_log_level: 'WARNING'  # Reduced from INFO to WARNING for performance
  save_metrics_to_file: true
  metrics_file: 'logs/pretraining/federated_pretraining_metrics.json'
  log_metrics_every_n_steps: 20  # Increased from 5 to 20 for performance

simulation:
  num_supernodes: 10

# Checkpointing configuration
checkpointing:
  save_dir: '/home/saadan/scratch/federated_librispeech/src/checkpoints/pretraining'
  save_latest: false         # Don't save latest separately (part of 3-checkpoint strategy)
  save_best: true            # Save best model based on validation loss
  save_best_round: true      # Save checkpoint for the current round
  cleanup_old: true          # Remove old round checkpoints to maintain exactly 3
  max_checkpoints: 3         # Exactly 3 checkpoints: best, current round, previous round

# Research Standards Compliance
# This configuration follows the original HuBERT paper:
# - hidden_size: 768 (divisible by 12 attention heads)
# - num_attention_heads: 12 (standard for transformer models)
# - num_hidden_layers: 12 (HuBERT base architecture)
# - intermediate_size: 3072 (4x hidden_size as per transformer standard)
# - vocab_size: 504 (K-means clustering vocabulary)
# - frame_stride: 320 (audio frame processing)
# - activation: GELU (as used in HuBERT)
# - dropout: 0.1 (standard regularization)
