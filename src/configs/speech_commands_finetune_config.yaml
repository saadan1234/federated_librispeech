# Speech Commands Finetuning Configuration for HuBERT Model
# Optimized for Compute Canada resources and following s3prl structure

# Model configuration - required for loading pretrained model
model:
  hidden_size: 768
  num_layers: 12
  frame_stride: 320

runner:
  total_steps: 200000
  gradient_clipping: 1.0
  gradient_accumulate_steps: 1
  
  log_step: 100
  eval_step: 5000
  save_step: 1000
  max_keep: 1
  eval_dataloaders:
    - dev
    - test

optimizer:
  name: TorchOptim
  torch_optim_name: Adam
  lr: 1.0e-4
  weight_decay: 0.01

# comment the whole scheduler config block
# to disable learning rate scheduling
# scheduler:
#   name: linear_schedule_with_warmup
#   num_warmup_steps: 1400

downstream_expert:
  datarc:
    speech_commands_root: /home/saadan/scratch/federated_librispeech/speech_commands
    speech_commands_test_root: /home/saadan/scratch/federated_librispeech/speech_commands
    num_workers: 8
    batch_size: 32
    sample_rate: 16000
    max_length: 16000  # 1 second at 16kHz
    
    # Data augmentation (optional)
    spec_augment: false
    time_shift: false
    background_noise: false

  modelrc:
    projector_dim: 256
    select: UtteranceLevel
    UtteranceLevel:
      pooling: MeanPooling
      dropout: 0.1
      hidden_size: 768
