# Research Paper Compliant Federated HuBERT Self-Supervised Pretraining Configuration
# Architecture matches standard HuBERT with federated training setup

# ============================================================================
# FLOWER SIMULATION CONFIGURATION  
# ============================================================================
simulation:
  # 10 clients as specified in research paper
  num_supernodes: 10
  
  # Backend configuration for Ray (Flower's simulation backend)
  backend:
    name: "ray"
    config:
      # Optimized resource allocation per client
      client_resources:
        num_cpus: 1.0      # Full CPU per client
        num_gpus: 0.5      # 2 clients per GPU
        memory: 6_000_000_000  # 6GB RAM per client
      
      # Ray initialization arguments
      init_args:
        log_to_driver: false
        configure_logging: true
        local_mode: false
        logging_level: 30
        
  # Enable TensorFlow GPU growth if using TF components
  enable_tf_gpu_growth: false
  
  # Reduced logging for speed
  verbose_logging: false

# ============================================================================
# FEDERATED LEARNING STRATEGY CONFIGURATION
# ============================================================================
strategy:
  name: "FedAdam"  # Using FedAdam as specified in paper
  
  # Client selection parameters for 10 clients
  fraction_fit: 0.8          # 80% of clients for training (8 clients)
  fraction_evaluate: 0.6     # 60% of clients for evaluation
  min_fit_clients: 8         # Minimum clients required for training
  min_evaluate_clients: 5    # Minimum clients required for evaluation
  min_available_clients: 10  # All 10 clients should be available
  
  # Server-side evaluation (centralized evaluation)
  evaluate_on_server: false
  
  # Custom aggregation functions
  fit_metrics_aggregation_fn: "weighted_average"
  evaluate_metrics_aggregation_fn: "weighted_average"

# ============================================================================
# PRETRAINING CONFIGURATION - RESEARCH PAPER SPECIFICATIONS
# ============================================================================
pretraining:
  # 20 federated rounds as specified in paper
  num_rounds: 20
  
  # 5 local epochs per client as specified in paper
  local_epochs: 5
  batch_size: 8
  learning_rate: 5e-4
  warmup_steps: 1000
  max_grad_norm: 1.0
  weight_decay: 0.01
  
  # Audio processing parameters
  max_audio_length: 160000   # 10 seconds at 16kHz
  sample_rate: 16000
  
  # Masking parameters as specified in paper
  mask_prob: 0.08           # 0.08 probability as specified
  mask_length: 10           # Span length 10 as specified
  
  # Standard HuBERT architecture as specified in paper
  vocab_size: 504           # k-means cluster assignments (504 units)
  hidden_size: 768          # 768 dimensional space
  num_hidden_layers: 12     # 12-layer Transformer encoder
  num_attention_heads: 12   # 12 attention heads
  intermediate_size: 3072   # 3072-dimensional feedforward networks
  
  # Model checkpointing
  save_model_every_n_rounds: 5
  save_best_model: true
  
  # Checkpointing configuration
  checkpoint_dir: "checkpoints/pretraining"
  save_checkpoints: true
  checkpoint_freq: 1
  keep_last_n_checkpoints: 3
  resume_from_checkpoint: false

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  # LibriSpeech dataset configuration
  dataset_name: "librispeech"
  subset: 
    - "train-clean-100"
    - "train-clean-360" 
    - "train-other-500"
  
  # Data paths
  librispeech_source: "/lustre07/scratch/saadan/federated_librispeech/LibriSpeechTars/LibriSpeech"
  partitioned_data_root: "federated_librispeech/data"
  global_test_path: "/lustre07/scratch/saadan/federated_librispeech/LibriSpeechTars/LibriSpeech/test-clean"
  global_validation_path: "/lustre07/scratch/saadan/federated_librispeech/LibriSpeechTars/LibriSpeech/dev-clean"
  
  # Speaker-based non-IID partitioning as specified in paper
  partition_method: "speaker_based"
  validation_split: 0.1
  test_split: 0.1
  iid_distribution: false
  
  # Data preprocessing
  preprocessing:
    normalize_audio: true
    remove_silence: false
    augmentation: false
    
  # DataLoader configuration
  dataloader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 3
    persistent_workers: true

# ============================================================================
# CLIENT CONFIGURATION
# ============================================================================
client:
  # Client-specific settings
  client_id_start: 0
  
  # Local training configuration (5 epochs as specified)
  local_config:
    epochs: 5
    batch_size: 8
    shuffle: true
    drop_last: true
    
  # Client evaluation
  evaluate_locally: true
  evaluation_fraction: 0.1
  
  # Resource constraints
  max_memory_usage: "6GB"
  enable_gpu: true
  mixed_precision: true

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================
server:
  # Server app configuration
  round_timeout: 1200  # 20 minutes timeout per round
  
  # Model aggregation with weighted averaging
  aggregation:
    weighted_averaging: true
    clip_norm: 1.0
    
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 0.001
    monitor: "eval_pretrain_loss"

# ============================================================================
# LOGGING AND MONITORING
# ============================================================================
logging:
  # Log directories
  log_dir: "logs/pretraining"
  server_log_dir: "logs/pretraining/server"
  client_log_dir: "logs/pretraining/clients"
  
  # Log levels
  server_log_level: "INFO"
  client_log_level: "INFO"
  framework_log_level: "WARNING"
  
  # Metrics logging
  log_metrics_every_n_steps: 5
  save_metrics_to_file: true
  metrics_file: "logs/pretraining/federated_pretraining_metrics.json"
  
  # Weights & Biases integration (optional)
  wandb:
    enabled: false
    project: "federated-hubert-pretraining"
    entity: "your-wandb-username"
    tags: ["federated", "librispeech", "hubert", "pretraining", "research"]

# ============================================================================
# CHECKPOINT AND SAVE CONFIGURATION
# ============================================================================
checkpointing:
  # Model saving
  save_dir: "checkpoints/pretraining"
  save_frequency: 1
  max_checkpoints_to_keep: 3
  save_best: true
  save_latest: true
  
  # Resume training
  resume_from_checkpoint: "checkpoints/pretraining/latest.pt"
  
  # Final model
  final_model_path: "/home/saadan/scratch/federated_librispeech/src/models/pretrained_hubert.pt"

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  # Evaluation metrics for pretraining
  metrics: ["loss", "perplexity"]
  
  # Evaluation frequency
  evaluate_every_n_rounds: 5
  final_evaluation: true
  
  # Pretraining-specific evaluation
  pretraining_evaluation:
    enabled: true
    eval_rounds: [5, 10, 15, 20]

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false

# ============================================================================
# EXPERIMENTAL FEATURES
# ============================================================================
experimental:
  # Gradient compression for faster communication
  gradient_compression:
    enabled: false
    method: "topk"
    compression_ratio: 0.1
  
  # Model compression
  model_compression:
    enabled: false
    method: "quantization"
  
  # Advanced masking strategies
  advanced_masking:
    enabled: false
    dynamic_masking: false
    contrastive_learning: false
    
  # Federated optimization
  federated_optimization:
    adaptive_aggregation: false
    client_drift_regularization: false