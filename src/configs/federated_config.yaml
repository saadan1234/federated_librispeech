# Federated LibriSpeech Configuration for Flower Simulation
# Optimized for HuBERT model training with run_simulation

# ============================================================================
# FLOWER SIMULATION CONFIGURATION
# ============================================================================
simulation:
  # Number of clients (supernodes) to simulate
  num_supernodes: 10
  
  # Backend configuration for Ray (Flower's simulation backend)
  backend:
    name: "ray"
    config:
      # Resource allocation per client
      client_resources:
        num_cpus: 2.0      # CPUs per client
        num_gpus: 0.25     # GPU fraction per client (4 clients can share 1 GPU)
        memory: 8_000_000_000  # 8GB RAM per client
      
      # Ray initialization arguments
      init_args:
        log_to_driver: false
        configure_logging: true
        local_mode: false    # Set to true for debugging single client
        
  # Enable TensorFlow GPU growth if using TF components
  enable_tf_gpu_growth: false
  
  # Verbose logging for debugging
  verbose_logging: true

# ============================================================================
# FEDERATED LEARNING STRATEGY CONFIGURATION
# ============================================================================
strategy:
  name: "FedAvg"  # Available: FedAvg, FedAdagrad, FedAdam, FedYogi, etc.
  
  # Client selection parameters
  fraction_fit: 0.8          # Fraction of clients for training (80%)
  fraction_evaluate: 0.6     # Fraction of clients for evaluation (60%)
  min_fit_clients: 8         # Minimum clients required for training
  min_evaluate_clients: 5    # Minimum clients required for evaluation
  min_available_clients: 10  # Minimum total clients to start training
  
  # Server-side evaluation (centralized evaluation)
  evaluate_on_server: true
  centralized_evaluation_config:
    server_round_schedule: [1, 5, 10, 15, 20]  # Evaluate on these rounds
  
  # Custom aggregation functions
  fit_metrics_aggregation_fn: "weighted_average"
  evaluate_metrics_aggregation_fn: "weighted_average"

# ============================================================================
# TRAINING CONFIGURATION
# ============================================================================
training:
  # Total number of federated rounds
  num_rounds: 20
  
  # Local training parameters
  local_epochs: 3
  batch_size: 16
  learning_rate: 5e-5
  warmup_steps: 500
  max_grad_norm: 1.0
  
  # Audio processing parameters
  max_audio_length: 160000   # 10 seconds at 16kHz
  sample_rate: 16000
  
  # Model checkpointing
  save_model_every_n_rounds: 5
  save_best_model: true

# ============================================================================
# MODEL CONFIGURATION (HuBERT)
# ============================================================================
model:
  # HuBERT model configuration
  name: "hubert"
  model_name_or_path: "facebook/hubert-base-ls960"
  
  # Model parameters
  config:
    freeze_feature_encoder: false
    freeze_base_model: false
    gradient_checkpointing: true
    
  # Fine-tuning configuration
  fine_tuning:
    target_modules: ["q_proj", "v_proj", "out_proj"]  # For LoRA if used
    task_type: "FEATURE_EXTRACTION"
    
  # Optimization
  optimizer:
    name: "adamw"
    lr: 5e-5
    weight_decay: 0.01
    eps: 1e-8
    
  scheduler:
    name: "linear"
    warmup_ratio: 0.1

# ============================================================================
# DATA CONFIGURATION
# ============================================================================
data:
  # LibriSpeech dataset configuration
  dataset_name: "librispeech"
  subset: ["train-clean-100", "train-clean-360", "train-other-500"]
  
  # Data paths
  librispeech_source: "/lustre07/scratch/saadan/federated_librispeech/LibriSpeechTars/LibriSpeech"
  partitioned_data_root: "federated_librispeech/data"
  global_test_path: "/lustre07/scratch/saadan/federated_librispeech/LibriSpeechTars/LibriSpeech/test-clean"
  global_validation_path: "/lustre07/scratch/saadan/federated_librispeech/LibriSpeechTars/LibriSpeech/dev-clean"
  
  # Data partitioning
  partition_method: "speaker_based"
  validation_split: 0.1
  test_split: 0.1
  iid_distribution: false
  
  # Data preprocessing
  preprocessing:
    normalize_audio: true
    remove_silence: false
    augmentation: false
    
  # DataLoader configuration
  dataloader:
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2

# ============================================================================
# CLIENT CONFIGURATION
# ============================================================================
client:
  # Client-specific settings
  client_id_start: 0
  
  # Local training configuration
  local_config:
    epochs: 3
    batch_size: 16
    shuffle: true
    drop_last: false
    
  # Client evaluation
  evaluate_locally: true
  evaluation_fraction: 0.2  # Use 20% of local data for evaluation
  
  # Resource constraints
  max_memory_usage: "8GB"
  enable_gpu: true
  mixed_precision: true

# ============================================================================
# SERVER CONFIGURATION
# ============================================================================
server:
  # Server app configuration
  round_timeout: 600  # 10 minutes timeout per round
  
  # Model aggregation
  aggregation:
    weighted_averaging: true
    clip_norm: 1.0
    
  # Server evaluation dataset
  evaluation_dataset: "data/global/validation"
  test_dataset: "data/global/test"
  
  # Early stopping
  early_stopping:
    enabled: true
    patience: 5
    min_delta: 0.001
    monitor: "eval_loss"

# ============================================================================
# LOGGING AND MONITORING
# ============================================================================
logging:
  # Log directories
  log_dir: "logs"
  server_log_dir: "logs/server"
  client_log_dir: "logs/clients"
  
  # Log levels
  server_log_level: "INFO"
  client_log_level: "INFO"
  framework_log_level: "WARNING"
  
  # Metrics logging
  log_metrics_every_n_steps: 50
  save_metrics_to_file: true
  metrics_file: "logs/federated_metrics.json"
  
  # Weights & Biases integration (optional)
  wandb:
    enabled: false
    project: "federated-librispeech-hubert"
    entity: "your-wandb-username"
    tags: ["federated", "librispeech", "hubert"]

# ============================================================================
# CHECKPOINT AND SAVE CONFIGURATION
# ============================================================================
checkpointing:
  # Model saving
  save_dir: "checkpoints"
  save_frequency: 5  # Save every 5 rounds
  max_checkpoints_to_keep: 3
  
  # Resume training
  resume_from_checkpoint: null
  
  # Final model
  final_model_path: "models/final_federated_hubert.pt"

# ============================================================================
# EVALUATION CONFIGURATION
# ============================================================================
evaluation:
  # Evaluation metrics
  metrics: ["loss", "accuracy", "f1_score"]
  
  # Evaluation schedule
  evaluate_every_n_rounds: 1
  final_evaluation: true
  
  # Test set evaluation
  test_evaluation:
    enabled: true
    test_rounds: [10, 15, 20]  # Evaluate on test set at these rounds

# ============================================================================
# PRIVACY AND SECURITY (Future Extensions)
# ============================================================================
privacy:
  # Differential Privacy (if needed)
  differential_privacy:
    enabled: false
    noise_multiplier: 1.0
    max_grad_norm: 1.0
    
  # Secure Aggregation (if needed)
  secure_aggregation:
    enabled: false

# ============================================================================
# REPRODUCIBILITY
# ============================================================================
reproducibility:
  seed: 42
  deterministic: true
  benchmark: false

# ============================================================================
# EXPERIMENTAL FEATURES
# ============================================================================
experimental:
  # Custom client sampling
  custom_client_sampling: false
  
  # Adaptive learning rates
  adaptive_lr: false
  
  # Model compression
  model_compression:
    enabled: false
    method: "quantization"  # or "pruning", "distillation"