checkpointing:
  final_model_path: /home/saadan/scratch/federated_librispeech/src/models/distilled_hubert.pt
  max_checkpoints_to_keep: 3
  save_dir: /home/saadan/scratch/federated_librispeech/src/checkpoints/distillation
  save_frequency: 5
client:
  gradient_clipping: 1.0
  mixed_precision: true
data:
  dataloader:
    num_workers: 0
    persistent_workers: true
    pin_memory: true
  max_length: 160000
  partitioned_data_root: /home/saadan/scratch/federated_librispeech/src/federated_librispeech/data
  sample_rate: 16000
distillation:
  alpha: 0.7
  batch_size: 2
  beta: 0.3
  learning_rate: 5e-5
  local_epochs: 1
  mask_length: 10
  mask_prob: 0.15
  max_audio_length: 80000
  num_rounds: 20
  student_hidden_size: 384
  student_intermediate_size: 1536
  student_num_heads: 6
  student_num_layers: 3
  teacher_hidden_size: 768
  teacher_intermediate_size: 3072
  teacher_num_heads: 12
  teacher_num_layers: 12
  temperature: 4.0
  vocab_size: 504
  weight_decay: 0.01
early_stopping:
  enabled: true
  min_delta: 0.001
  monitor: eval_student_loss
  patience: 5
server:
  round_timeout: 600
simulation:
  backend:
    config:
      actor:
        tensorflow: 0
      client_resources:
        memory: 2147483648  # Reduced to 2GB
        num_cpus: 0.4
        num_gpus: 0.1
      init_args:
        configure_logging: true
        local_mode: false
        log_to_driver: false
        logging_level: 30
        num_cpus: 4
        num_gpus: 1
        object_store_memory: 10307921510
    type: ray
  num_supernodes: 5
strategy:
  fraction_evaluate: 1.0
  fraction_fit: 1.0
  min_available_clients: 3
  min_evaluate_clients: 3
  min_fit_clients: 3
teacher_weights_path: models/pretrained_hubert.pt
